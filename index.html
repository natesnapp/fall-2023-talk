<!DOCTYPE html>
<html>
  <head>
    <title>Using Ray and Alpa for large model training and serving</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Abstract

Optimizing model training and serving for large-scale models with billions of parameters poses significant challenges in terms of resource limitations, long training times, and efficient model serving. This presentation explores how Ray and Alpa can address these challenges and simplify the process of working with large models.

???

Notes for the _first_ slide

---

Topics

1. Ray Overview
2. Alpa Overview
3. Challenges of Large Model Training
4. How to Partition Large Models
5. Using Ray and Alpa to Simplify Training
6. Challenges of Large Model Serving
7. Ray and Alpa Optimizations for Model Serving
8. Some Shortcomings

---

# Ray Overview

* Open-source framework for building distributed applications
* Provides tools and libraries for scalable and efficient computing
* Enables distributed model training and serving

---

# Alpa Overview

* AI platform by Alpa, designed for large-scale machine learning
* Built on top of Ray for scalable and efficient training and serving
* Provides high-level abstractions for distributed model development

---

# Challenges of Large Model Training

* How to partition a computational graph
* How to handle heterogeneous network topology
* Long training times due to the sheer scale of data and computations

---

# How to Partition Large Models

* Splitting large language models (LLMs) into smaller segments
* Each segment processes a subset of the model's parameters (MoE, etc)
* Mapping distributed training across multiple 

---

# Simplifying Training with Ray and Alpa

* Ray provides distributed computing capabilities for efficient training
* Alpa offers high-level abstractions for distributed model development
* Simplifies the process of scaling training to large models

---

# Challenges of Large Model Serving

* Serving large models requires handling high computational demands
* Memory constraints when loading and running models
* Efficient and low-latency predictions for real-time applications

---

# Enabling Efficient Model Serving with Ray and Alpa

* Ray enables distributed model serving for efficient inference
* Alpa provides tools for deploying and managing large models in production
* Allows scaling model serving horizontally for high throughput

---

# Alpa on Ray benchmark results

* coming soon

---

# Shortcomings of Ray and Alpa for Large Model Training/Serving

* Workflows and specific instances where partitioning and scaling are not optimal
* Cases where annotation is not straight forward

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create();
    </script>
  </body>
</html>
